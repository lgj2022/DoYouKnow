

Selenium 크롤링 및 자동화 확인



```
from nltk import word_tokenize, pos_tag, ne_chunk
from nltk import RegexpParser
from nltk import Tree
import nltk
import re
import pandas as pd
import spacy
import time
from konlpy.tag import Okt
from nltk.corpus import stopwords
from urllib.parse import quote_plus
from bs4 import BeautifulSoup
from selenium import webdriver
import googletrans

#python -m spacy download ja_core_news_sm
#ja_core_news_sm
nlp = spacy.load("ko_core_news_sm")

def news(news_urls):
    translator = googletrans.Translator()
    resultsentence = []
    result = ""
    for url_no in news_urls:
        print(url_no)
        driver = webdriver.Chrome()
        driver.get(url_no)
        html = driver.page_source
        soup = BeautifulSoup(html)

        str = ""
        str2 = ""
        for text in soup.findAll("p"):
            str += text.get_text() + "\n"
            result+=str
            str=""
            if len(result)>1000:
                resultsentence.append(result)
                result=""
            
    return resultsentence



def np_tag(text):
    #text = re.sub('[^a-zA-Z]', ' ', text)
    #text = text.lower()
    hannanum = Okt()
    df = pd.DataFrame(columns = ['CHUNK'])
    doc = hannanum.nouns(text)
    news_desc = ' '.join(doc)
    print(news_desc)
    #for chunk in doc.noun_chunks:
    #    df = df.append({'CHUNK': chunk.text}, ignore_index=True)
    #for token in doc:
    #    print(token.text, token.pos_, token.dep_)    
    return df




testnews = ['https://www.goal.com/jp/%E3%83%AA%E3%82%B9%E3%83%88/from-heung-min-son-to-kai-havertz-how-bayer-leverkusen-made/bltaab0891b0db9583f']
#wl =np_tag(news(testnews))
wl = news(testnews)

print(wl)

#stemmer = nltk.stem.SnowballStemmer('english')
driver = webdriver.Chrome()
driver.get('https://translate.google.co.kr/?hl=ko&sl=auto&tl=ko&op=translate')
time.sleep(1)
input=driver.find_element("xpath",'//*[@id="yDmH0d"]/c-wiz/div/div[2]/c-wiz/div[2]/c-wiz/div[1]/div[2]/div[3]/c-wiz[1]/span/span/div/textarea')
for a in wl:
    input.send_keys(a[:2000])
    time.sleep(2)
    output=driver.find_element("xpath",'//*[@id="yDmH0d"]/c-wiz/div/div[2]/c-wiz/div[2]/c-wiz/div[1]/div[2]/div[3]/c-wiz[2]/div[8]/div/div[1]/span[1]/span/span')
    time.sleep(2)
    print(output.text)
time.sleep(10)
#testmemo =""
# f = open("C:/Users/multicampus/Desktop/test/test.txt", 'w', encoding='utf-8')
# for no_capital in no_capitals:
#     no_stops = [word for word in no_capital if not word in stops]
#     for tmp in no_stops:
#         #testmemo=testmemo+tmp+'\n'
#         f.write(tmp+'\n')
# f.close()
```

